import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –æ—à–∏–±–∫–∏ –≤ –∫–æ–Ω—Å–æ–ª–∏
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AITextDetector:
    def __init__(self):
        self.model_name = "Hello-SimpleAI/chatgpt-detector-roberta"
        self.is_loaded = False
        
        logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {self.model_name}...")
        try:
            # –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: GPU (cuda), Apple Silicon (mps) –∏–ª–∏ CPU
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
            elif torch.backends.mps.is_available():
                self.device = torch.device("mps") # –î–ª—è Mac M1/M2
            else:
                self.device = torch.device("cpu")
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
            
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")
            self.is_loaded = True
        except Exception as e:
            logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ó–ê–ì–†–£–ó–ö–ò: {e}")
            self.is_loaded = False

    def predict(self, text):
        if not self.is_loaded:
            return "–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞", 0.0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
        if not isinstance(text, str):
            return "–û—à–∏–±–∫–∞: –ü—Ä–∏—Å–ª–∞–Ω –Ω–µ —Ç–µ–∫—Å—Ç", 0.0

        if len(text.strip()) < 10:
            return "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π (–Ω—É–∂–Ω–æ > 10 —Å–∏–º–≤–æ–ª–æ–≤)", 0.0

        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=512
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            probabilities = F.softmax(logits, dim=-1)
            
            # –í —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –∏–Ω–¥–µ–∫—Å 1 - —ç—Ç–æ AI, –∏–Ω–¥–µ–∫—Å 0 - Human
            ai_probability = probabilities[0][1].item() * 100
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –≤–µ—Ä–¥–∏–∫—Ç
            if ai_probability > 80:
                verdict = "ü§ñ –≠—Ç–æ —Ç–æ—á–Ω–æ –ò–ò"
            elif ai_probability > 50:
                verdict = "ü§ñ –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ò–ò"
            else:
                verdict = "üë§ –¢–µ–∫—Å—Ç –Ω–∞–ø–∏—Å–∞–ª —á–µ–ª–æ–≤–µ–∫"
                
            return verdict, round(ai_probability, 1)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞: {e}")
            return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ", 0.0